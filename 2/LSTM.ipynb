{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from operator import mod\n",
    "from pandas.core.algorithms import mode\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class LstmModel(nn.Module):\n",
    "    def __init__(self, inputSize=5, hiddenSize=6):\n",
    "        super().__init__()\n",
    "        # LSTM层-> 两个LSTM单元叠加\n",
    "        self.lstm = nn.LSTM(input_size=inputSize, \\\n",
    "                            hidden_size=hiddenSize, num_layers=2)\n",
    "        self.output = nn.Linear(6, 2)  # 线性输出\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input->(time_step, batch, input_size)\n",
    "        x1, _ = self.lstm(x)\n",
    "        # x1: output->(time_step, batch, output_size)\n",
    "        a, b, c = x1.shape\n",
    "        out = self.output(x1.view(-1, c))  # 只有三维数据转化为二维才能作为输入\n",
    "        # 重新将结果转化为三维\n",
    "        out = out.view(a, b, -1)\n",
    "        return out\n",
    "\n",
    "# 训练模型\n",
    "def training_loop(nEpochs, model, optimizer, lossFn, trainData, testData=None):\n",
    "    trainX, trainY = trainData\n",
    "    if testData is not None:\n",
    "        testX, testY = testData\n",
    "    for epoch in range(1, nEpochs + 1):\n",
    "        optimizer.zero_grad()  # 梯度清0\n",
    "        trainP = model(trainX)\n",
    "        loss = lossFn(trainP, trainY)\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()\n",
    "        loss_.append(loss.item())\n",
    "\n",
    "        # 计算 预测的acc\n",
    "        trainP_ = trainP.view(-1, 2)\n",
    "        trainY_ = trainY.view(-1, 2)\n",
    "        _, trainP_ = torch.max(trainP_, 1)\n",
    "        _, trainY_ = torch.max(trainY_, 1)\n",
    "        acc = torch.sum(trainP_ == trainY_).item() / len(trainY)\n",
    "        acc_.append(acc)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "    Word  1 try  2 tries  3 tries  4 tries  5 tries  6 tries  \\\n0  slump      1        3       23       39       24        9   \n1  crank      1        5       23       31       24       14   \n2  gorge      1        3       13       27       30       22   \n3  query      1        4       16       30       30       17   \n4  drink      1        9       35       34       16        5   \n\n   7 or more tries (X)  syllable  frequency  tag  \n0                    1         1       5078    8  \n1                    2         1       3506    8  \n2                    4         1       2019    8  \n3                    2         2       2587    8  \n4                    1         1      54651   13  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>1 try</th>\n      <th>2 tries</th>\n      <th>3 tries</th>\n      <th>4 tries</th>\n      <th>5 tries</th>\n      <th>6 tries</th>\n      <th>7 or more tries (X)</th>\n      <th>syllable</th>\n      <th>frequency</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>slump</td>\n      <td>1</td>\n      <td>3</td>\n      <td>23</td>\n      <td>39</td>\n      <td>24</td>\n      <td>9</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5078</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>crank</td>\n      <td>1</td>\n      <td>5</td>\n      <td>23</td>\n      <td>31</td>\n      <td>24</td>\n      <td>14</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3506</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>gorge</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>27</td>\n      <td>30</td>\n      <td>22</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2019</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>query</td>\n      <td>1</td>\n      <td>4</td>\n      <td>16</td>\n      <td>30</td>\n      <td>30</td>\n      <td>17</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2587</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>drink</td>\n      <td>1</td>\n      <td>9</td>\n      <td>35</td>\n      <td>34</td>\n      <td>16</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>54651</td>\n      <td>13</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取源数据\n",
    "source_data = pd.read_csv('../data/source_data.csv', usecols=[2, 5, 6, 7, 8, 9, 10, 11])\n",
    "handle_data = pd.read_csv('../data/handle_data.csv', usecols=[0, 2, 3, 4])\n",
    "# 将两者合并\n",
    "data = pd.merge(source_data, handle_data)\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 输入1： 单词编码\n",
    "def word2index(word):\n",
    "    index_list = []\n",
    "    # 定义 a 到 z 的字母表对于数字 1-26 的映射\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    for i in word:\n",
    "        value = alphabet.index(i) + 1\n",
    "        # 归一化value\n",
    "        value = (value-1) / 25\n",
    "        index_list.append(value)\n",
    "\n",
    "    return index_list\n",
    "\n",
    "# 输入2： 频率编码\n",
    "def freq_encode(frequency):\n",
    "    freq = (frequency - data['frequency'].min()) / (data['frequency'].max() - data['frequency'].min())\n",
    "    return freq\n",
    "\n",
    "# 输入3： 词性编码\n",
    "def pos_encode(pos):\n",
    "    pos = (pos - data['tag'].min()) / (data['tag'].max() - data['tag'].min())\n",
    "    return pos\n",
    "\n",
    "# 输入4： 音节编码\n",
    "def syllable_encode(syllable):\n",
    "    syllable = (syllable - data['syllable'].min()) / (data['syllable'].max() - data['syllable'].min())\n",
    "    return syllable\n",
    "\n",
    "# 输出\n",
    "def norm_(y_s):\n",
    "    temp = [1, 2, 3, 4, 5, 6, 7]\n",
    "    # data_为0个1， 1个2， 9个3， 29个4， 34个5， 22个6， 5个7\n",
    "    datas = []\n",
    "    for i, j in enumerate(y_s):\n",
    "        datas.extend([temp[i]] * j)\n",
    "    mu = np.mean(datas)\n",
    "    sigma = np.std(datas)\n",
    "    return mu, sigma, datas\n",
    "\n",
    "# 划分训练集和测试集\n",
    "# 由于是时间序列数据，不适合这样随机打乱\n",
    "def splitData(data, rate=0.7):\n",
    "    # 默认训练集比例为0.7\n",
    "    dataX, dataY = data\n",
    "    nSample = dataX.shape[0]\n",
    "    nTrain = int(nSample * rate)\n",
    "    # shuffledIndices = torch.randperm(nSample)\n",
    "    trainData = (dataX[:nTrain], dataY[:nTrain])\n",
    "    testData = (dataX[nTrain:], dataY[nTrain:])\n",
    "    return trainData, testData\n",
    "\n",
    "import math\n",
    "def normal_distribution(x, mean, sigma):\n",
    "    return np.exp(-1*((x-mean)**2)/(2*(sigma**2)))/(math.sqrt(2*np.pi)* sigma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 输入数据转化\n",
    "dataX = []\n",
    "for i in range(len(data)):\n",
    "    word = data['Word'][i]\n",
    "    dataX.append(word2index(word))\n",
    "    freq = freq_encode(data['frequency'][i])\n",
    "    dataX[i].append(freq)\n",
    "    pos = pos_encode(data['tag'][i])\n",
    "    dataX[i].append(pos)\n",
    "    syllable = syllable_encode(data['syllable'][i])\n",
    "    dataX[i].append(syllable)\n",
    "\n",
    "# 转化为tensor\n",
    "dataX = torch.tensor(dataX)\n",
    "dataX = dataX.reshape(-1, 1, 8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# 输出数据转化\n",
    "from scipy import stats\n",
    "dataY = []\n",
    "mus = []\n",
    "sigmas = []\n",
    "statistics = []\n",
    "pvalues = []\n",
    "datass = []\n",
    "for i in range(len(data)):\n",
    "    y_s = [data['1 try'][i], data['2 tries'][i], data['3 tries'][i], data['4 tries'][i], data['5 tries'][i], data['6 tries'][i], data['7 or more tries (X)'][i]]\n",
    "\n",
    "    mu, sigma, datas = norm_(y_s)\n",
    "    datass.append(datas)\n",
    "    mus.append(mu)\n",
    "    sigmas.append(sigma)\n",
    "    result = stats.shapiro(datas)\n",
    "    statistics.append(result[0])\n",
    "    pvalues.append(result[1])\n",
    "\n",
    "# mus, sigmas 归一化\n",
    "mus_ = (mus - np.min(mus)) / (np.max(mus) - np.min(mus))\n",
    "sigmas_ = (sigmas - np.min(sigmas)) / (np.max(sigmas) - np.min(sigmas))\n",
    "# 组合形成dataY\n",
    "for i in range(len(data)):\n",
    "    dataY.append([mus_[i], sigmas_[i]])\n",
    "\n",
    "# 转化为tensor\n",
    "dataY = torch.tensor(dataY)\n",
    "dataY = dataY.reshape(-1, 1, 2)\n",
    "# dataY\n",
    "datass\n",
    "pd.DataFrame(datass).T.to_csv('../data/datass.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 将数据类型都转变为 float32\n",
    "dataX = dataX.float()\n",
    "dataY = dataY.float()\n",
    "\n",
    "data_ = dataX, dataY\n",
    "# 获取训练集和测试集，用80%的数据来训练拟合，20%的数据来预测\n",
    "rate = 0.8\n",
    "trainData, testData = splitData(data_, rate=rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "lstm = LstmModel(inputSize=8)  # inputSize与look_back保持一致\n",
    "# 使用优化器Adam比SGD更好\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.1)\n",
    "loss_func = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#! 寻找mu和freq的相关性\n",
    "# freq = []\n",
    "# for i in range(len(data)):\n",
    "#     freq.append(freq_encode(data['frequency'][i]))\n",
    "#     dataX[i].append(freq)\n",
    "#\n",
    "# np.corrcoef(freq, mus_)\n",
    "#\n",
    "# # 将mus, sigmas, freq 组合起来形成 dataframe\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame({'mus': mus_, 'sigmas': sigmas_, 'freq': freq})\n",
    "# mapdata = df.corr(method='spearman')\n",
    "# # 画出热力图\n",
    "# import seaborn as sns\n",
    "# fig = plt.figure(dpi=400)\n",
    "# sns.heatmap(mapdata, annot=True, cmap='YlGnBu_r', vmax=1, square=True)\n",
    "# plt.show()\n",
    "# fig.savefig('corr.png', dpi=400)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m loss_ \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      3\u001B[0m acc_ \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m----> 4\u001B[0m lstm \u001B[38;5;241m=\u001B[39m \u001B[43mtraining_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnEpochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlossFn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainData\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrainData\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[3], line 27\u001B[0m, in \u001B[0;36mtraining_loop\u001B[1;34m(nEpochs, model, optimizer, lossFn, trainData, testData)\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, nEpochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m     26\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# 梯度清0\u001B[39;00m\n\u001B[1;32m---> 27\u001B[0m     trainP \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m     loss \u001B[38;5;241m=\u001B[39m lossFn(trainP, trainY)\n\u001B[0;32m     29\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()  \u001B[38;5;66;03m# 反向传播\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program1\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[3], line 12\u001B[0m, in \u001B[0;36mLstmModel.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m# x: input->(time_step, batch, input_size)\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m     x1, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;66;03m# x1: output->(time_step, batch, output_size)\u001B[39;00m\n\u001B[0;32m     14\u001B[0m     a, b, c \u001B[38;5;241m=\u001B[39m x1\u001B[38;5;241m.\u001B[39mshape\n",
      "File \u001B[1;32mC:\\Program1\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\Program1\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:774\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m    772\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_forward_args(\u001B[38;5;28minput\u001B[39m, hx, batch_sizes)\n\u001B[0;32m    773\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 774\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    775\u001B[0m \u001B[43m                      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_first\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    776\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    777\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\u001B[38;5;28minput\u001B[39m, batch_sizes, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias,\n\u001B[0;32m    778\u001B[0m                       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "loss_ = []\n",
    "acc_ = []\n",
    "lstm = training_loop(\n",
    "    nEpochs=1000,\n",
    "    model=lstm,\n",
    "    optimizer=optimizer,\n",
    "    lossFn=loss_func,\n",
    "    trainData=trainData)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 计算Accuracy\n",
    "# 计算测试集的预测值\n",
    "testX, testY = testData\n",
    "testY = testY.reshape(-1, 2)\n",
    "\n",
    "testX = testX.float()\n",
    "testY = testY.float()\n",
    "# trainX, trainY = trainData\n",
    "# trainX = trainX.float()\n",
    "# trainY = trainY.float()\n",
    "\n",
    "out = lstm(testX)\n",
    "out = out.view(-1, 2)\n",
    "out = out.detach().numpy()\n",
    "# 计算accuracy并绘制图像\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(testY, out)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 绘制loss曲线\n",
    "fig = plt.figure(dpi=400)\n",
    "plt.plot(loss_)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "fig.savefig('loss.png', dpi=400)\n",
    "\n",
    "# 绘制acc曲线\n",
    "fig = plt.figure(dpi=400)\n",
    "plt.plot(acc_)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "fig.savefig('acc.png', dpi=400)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 求acc的最高值的index\n",
    "max_acc_index = 0\n",
    "\n",
    "max_acc = 0\n",
    "for i in range(len(acc_)):\n",
    "    if acc_[i] > max_acc:\n",
    "        max_acc = acc_[i]\n",
    "        max_acc_index = i\n",
    "print('max_acc: ', max_acc)\n",
    "print('max_acc_index: ', max_acc_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_list = word2index('eerie')\n",
    "freq = freq_encode(2272)\n",
    "pos = pos_encode(8)\n",
    "syllable = syllable_encode(2)\n",
    "data_list.append(freq)\n",
    "data_list.append(pos)\n",
    "data_list.append(syllable)\n",
    "\n",
    "data_list = torch.tensor(data_list)\n",
    "data_list = data_list.reshape(-1, 1, 8)\n",
    "data_list = data_list.float()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# 将out 转变为原始数据\n",
    "# out = lstm(data_list)\n",
    "# out = out.view(-1).data.numpy()\n",
    "out = tensor([[[0.5331, 0.5688]]])\n",
    "out = out.view(-1).data.numpy()\n",
    "out[0] = out[0] * (np.max(mus) - np.min(mus)) + np.min(mus)\n",
    "out[1] = out[1] * (np.max(sigmas) - np.min(sigmas)) + np.min(sigmas)\n",
    "# 转化为正态分布并绘制图像\n",
    "mu = out[0]\n",
    "sigma = out[1]\n",
    "\n",
    "# 分别计算正态分布在 1， 2， 3， 4， 5， 6， 7以上的概率，并绘制柱形图\n",
    "x = [1, 2, 3, 4, 5, 6, 7]\n",
    "y = []\n",
    "for i in range(1, 8):\n",
    "    y.append(round(normal_distribution(i, mu, sigma), 2))\n",
    "\n",
    "print(y)\n",
    "fig = plt.figure(dpi=400)\n",
    "plt.bar(x, y, width=0.5, alpha=0.5)\n",
    "plt.xlabel('Number of tries')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "\n",
    "\n",
    "print(mu, sigma)\n",
    "x = np.linspace(mu - 3 * sigma, mu + 3 * sigma, 50)\n",
    "y = norm.pdf(x, mu, sigma)\n",
    "plt.plot(x, y, 'r-', lw=2)\n",
    "plt.xlabel('Number of tries')\n",
    "plt.ylabel('Probability')\n",
    "fig.savefig('result2.png', dpi=400)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataX, dataY = data  # 原始数据 -> (time_step, batch, input_size)\n",
    "# dataY = dataY.view(-1).data.numpy()  # 展开为1维\n",
    "# dataY = dataY * (maxPassenger - minPassenger) + minPassenger\n",
    "# dataP = lstm(dataX)  # 进行拟合\n",
    "# dataP = dataP.view(-1).data.numpy()  # 展开为1维\n",
    "# dataP = dataP * (maxPassenger - minPassenger) + minPassenger"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# dataX, dataY = data  # 原始数据 -> (time_step, batch, input_size)\n",
    "# predicts = []\n",
    "# for i in range(30):\n",
    "#     data_last = dataX[-1, :, :]\n",
    "#     print(data_last)\n",
    "#     # 将data_last转化为三维数据\n",
    "#     data_last = data_last.reshape(1, 1, -1)\n",
    "#     predict = lstm(data_last)\n",
    "#     print(predict)\n",
    "#     # 将data_last后两个的数据前移,并加入预测值\n",
    "#     data_last = torch.cat((data_last[:, :, 1:], predict), dim=2)\n",
    "#\n",
    "#     dataX = torch.cat((dataX, data_last), dim=0)\n",
    "#     predict = predict.view(-1).data.numpy()\n",
    "#     predict = predict * (maxPassenger - minPassenger) + minPassenger\n",
    "#     predicts.append(predict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# nTrain = int(dataY.shape[0] * rate)  # 拟合的数量\n",
    "# nData = dataY.shape[0]  # 预测的数量\n",
    "#\n",
    "# # 绘制对比图\n",
    "# plt.rcParams['font.sans-serif'] = 'KaiTi'  # 正常显示中文\n",
    "# fig = plt.figure(dpi=400)\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.plot(np.arange(202, 202+360), dataY[:360], color='blue', label=\"Actual value\")\n",
    "# ax.plot(np.arange(202, 202+360), dataP[:360], color='orange', \\\n",
    "#         linestyle='--', label='Fitted value')\n",
    "# ax.plot(np.arange(202+360, 202+nData), dataP[360:], \\\n",
    "#         linestyle='--', color='red', label='Predictive value')\n",
    "# ax.legend()\n",
    "# plt.xlabel('Contest number',fontsize=14)\n",
    "# plt.ylabel('Number of  reported results',fontsize=14)\n",
    "#\n",
    "# fig.savefig('test.png', dpi=400)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
